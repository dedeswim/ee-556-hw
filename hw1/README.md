# EE-556 Homework 1

This folder contains the first homework. The homework is focused on [**SVNs**](https://en.wikipedia.org/wiki/Support-vector_machine), with smooth [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss#Optimization). Using just NumPy and the skeleton I have been given by the instructors, in this homework I implemented the following optimization algorithms:

- [x] Gradient Descent
- [x] Gradient Descent leveraging the objective function's strong convexity
- [x] Accelerated Gradient Descent
- [x] Accelerated Gradient Descent leveraging the objective function's strong convexity
- [x] Gradient Descent with line search
- [x] Accelerated Gradient Descent with line search
- [x] Accelerated Gradient Descent with restart
- [x] Accelerated Gradient Descent (line search + restart)
- [x] AdaGrad
- [x] ADAM
- [x] Stochastic Gradient Descent
- [x] Stochastic Averaging Gradient
- [x] Stochastic Gradient Descent with Variance Reduction

In the [report](report/) folder, you can find the report some proofs about the mathematical background of the smooth Hinge loss function (i.e. proof of the gradient, the Hessian and some other properties)
